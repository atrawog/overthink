# llama.cpp tools + vLLM cu130 install (post-pixi)
version: '3'

tasks:
  install:
    cmds:
      # Download llama.cpp prebuilt binaries + Python tooling
      - |
        LLAMA_CPP_DIR=~/llama.cpp
        mkdir -p "$LLAMA_CPP_DIR"
        LLAMA_RELEASE=$(curl -fsSL "https://api.github.com/repos/ggerganov/llama.cpp/releases/latest" | grep -oP '"tag_name": "\K[^"]+')
        curl -fsSL -o /tmp/llama-bin.tar.gz \
          "https://github.com/ggerganov/llama.cpp/releases/download/${LLAMA_RELEASE}/llama-${LLAMA_RELEASE}-bin-ubuntu-x64.tar.gz"
        mkdir -p /tmp/llama_extract
        tar --no-same-owner --no-same-permissions -xzf /tmp/llama-bin.tar.gz -C /tmp/llama_extract
        EXTRACT_DIR="/tmp/llama_extract/llama-${LLAMA_RELEASE}"
        cp "$EXTRACT_DIR/llama-quantize" "$EXTRACT_DIR/llama-cli" "$LLAMA_CPP_DIR/"
        chmod +x "$LLAMA_CPP_DIR/llama-quantize" "$LLAMA_CPP_DIR/llama-cli"
        cp "$EXTRACT_DIR"/lib*.so* "$EXTRACT_DIR/LICENSE" "$LLAMA_CPP_DIR/"
        rm -rf /tmp/llama_extract /tmp/llama-bin.tar.gz
      # Download llama.cpp source for convert_hf_to_gguf.py + gguf-py
      - |
        LLAMA_CPP_DIR=~/llama.cpp
        LLAMA_RELEASE=$(curl -fsSL "https://api.github.com/repos/ggerganov/llama.cpp/releases/latest" | grep -oP '"tag_name": "\K[^"]+')
        curl -fsSL -o /tmp/llama-src.tar.gz \
          "https://github.com/ggerganov/llama.cpp/archive/refs/tags/${LLAMA_RELEASE}.tar.gz"
        mkdir -p /tmp/llama_src
        tar --no-same-owner --no-same-permissions -xzf /tmp/llama-src.tar.gz -C /tmp/llama_src
        SRC_DIR="/tmp/llama_src/llama.cpp-${LLAMA_RELEASE}"
        cp "$SRC_DIR/convert_hf_to_gguf.py" "$LLAMA_CPP_DIR/"
        cp -r "$SRC_DIR/gguf-py" "$LLAMA_CPP_DIR/"
        rm -rf /tmp/llama_src /tmp/llama-src.tar.gz
      # Install vLLM 0.14.0 cu130 nightly (no conda/pixi package available)
      - |
        ~/.pixi/envs/default/bin/python -m pip install --no-deps \
          'https://wheels.vllm.ai/adcf682fc7d1835d037da331922751e880c8bc25/vllm-0.14.0rc1.dev201%2Bgadcf682fc.cu130-cp38-abi3-manylinux_2_35_x86_64.whl'
      # Install vLLM required dependencies
      - |
        ~/.pixi/envs/default/bin/python -m pip install \
          blake3 cbor2 'compressed-tensors==0.13.0' 'depyf==0.20.0' \
          'flashinfer-python==0.5.3' ijson 'llguidance>=1.3.0,<1.4.0' \
          'lm-format-enforcer==0.11.3' 'mistral_common>=1.8.5' \
          'model-hosting-container-standards>=0.1.9,<1.0.0' 'numba==0.61.2' \
          'openai-harmony>=0.0.3' 'opencv-python-headless>=4.11.0' \
          'outlines_core==0.2.11' partial-json-parser \
          'prometheus-fastapi-instrumentator>=7.0.0' 'ray>=2.48.0' \
          watchfiles 'xgrammar==0.1.29' 'lark==1.2.2' \
          pydantic-extra-types pycountry interegular hf_transfer \
          cut_cross_entropy torchao tyro loguru 'llvmlite>=0.44.0,<0.45' \
          apache-tvm-ffi nvidia-cudnn-frontend nvidia-cutlass-dsl
